from langchain_ollama import OllamaLLM, OllamaEmbeddings
from langchain_chroma import Chroma
from langchain_core.prompts import PromptTemplate

# åˆå§‹åŒ–åµŒå…¥æ¨¡å‹
embedding = OllamaEmbeddings(model="mxbai-embed-large")

# æ­£ç¢ºè®€å–å‘é‡è³‡æ–™åº«
vectorstore = Chroma(
    persist_directory="vector_db/knowledge_db",
    embedding_function=embedding  # âœ… æ­£ç¢ºå¯«æ³•æ˜¯ embedding_function
)

# å»ºç«‹ Retriever
retriever = vectorstore.as_retriever()

# åˆå§‹åŒ– LLM
llm = OllamaLLM(model="deepseek-r1:1.5b")

# æ¸¬è©¦å•é¡Œ
question = "ä»€éº¼æ˜¯é©åˆç©æ¥µå‹æŠ•è³‡äººçš„å¹£ç¨®é…ç½®ï¼Ÿ"

# æª¢ç´¢è³‡æ–™
docs = retriever.invoke(question)
context = "\n".join([doc.page_content for doc in docs])

# Prompt å»ºç«‹
prompt = PromptTemplate.from_template("""
ä½ æ˜¯åŠ å¯†è²¨å¹£æŠ•è³‡åŠ©ç†ï¼Œæ ¹æ“šä»¥ä¸‹è³‡æ–™å›ç­”ä½¿ç”¨è€…å•é¡Œã€‚

åƒè€ƒè³‡æ–™ï¼š
{context}

ä½¿ç”¨è€…å•é¡Œï¼š
{question}

è«‹ä»¥ç°¡æ½”ã€æ¸…æ¥šçš„æ–¹å¼å›ç­”ã€‚
""")

# çµ„åˆ Prompt ä¸¦ç”¢ç”Ÿå›ç­”
final_prompt = prompt.format(context=context, question=question)
answer = llm.invoke(final_prompt)

print("ğŸ“Œ å•é¡Œï¼š", question)
print("ğŸ¤– å›ç­”ï¼š", answer)
