import os
import sys
import django

# å¾€ä¸Šå…©å±¤æ‰åˆ° manage.py åŒå±¤çš„æ ¹ç›®éŒ„
BASE_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), "..", ".."))
sys.path.insert(0, BASE_DIR)

os.environ.setdefault("DJANGO_SETTINGS_MODULE", "cryptocurrency.settings")
django.setup()


from typing import Optional
from news.models import Article
from langchain_ollama import OllamaEmbeddings
from langchain_chroma import Chroma
from langchain_core.documents import Document
from langchain_ollama.llms import OllamaLLM
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.vectorstores import VectorStoreRetriever

def initialize_news_vector_store(
    db_location: str = "./vector_db/news",
    model_name: str = "mxbai-embed-large",
    max_docs: int = 100,
    test_query: Optional[str] = None
) -> Chroma:
    """
    åˆå§‹åŒ–æ–°èè³‡æ–™çš„å‘é‡è³‡æ–™åº«ï¼Œè‹¥å°šæœªå»ºç«‹å‰‡æœƒå¾è³‡æ–™åº«ä¸­å–å‡ºæ–°èå»ºç«‹å‘é‡ã€‚

    åƒæ•¸ï¼š
    - db_location: å„²å­˜å‘é‡è³‡æ–™åº«çš„è·¯å¾‘
    - model_name: ä½¿ç”¨çš„ Ollama åµŒå…¥æ¨¡å‹åç¨±
    - max_docs: åˆæ¬¡åŠ å…¥å‘é‡çš„æœ€å¤§æ–°èæ•¸é‡
    - test_query: è‹¥æä¾›å‰‡æœƒåŸ·è¡ŒæŸ¥è©¢æ¸¬è©¦

    å›å‚³ï¼š
    - vector_store: Chroma å‘é‡è³‡æ–™åº«å¯¦é«”
    """
    add_documents = not os.path.exists(db_location)

    embeddings = OllamaEmbeddings(model=model_name)

    vector_store = Chroma(
        collection_name="crypto_news_articles",
        persist_directory=db_location,
        embedding_function=embeddings,
    )

    if add_documents:
        documents = []
        ids = []

        articles = Article.objects.filter(
            summary__isnull=False,
            content__isnull=False
        ).order_by("-time")[:max_docs]

        for article in articles:
            doc = Document(
                page_content=f"{article.title or ''}",
                metadata={
                    "url": article.url,
                    "date": str(article.time),
                    "website": article.website.name,
                },
                id=str(article.id)
            )
            documents.append(doc)
            ids.append(str(article.id))

        print(f"ğŸ§  å‘é‡åŒ– {len(documents)} ç¯‡æ–°èè³‡æ–™ä¸­...")
        vector_store.add_documents(documents=documents, ids=ids)
        print("âœ… æˆåŠŸå»ºç«‹å§¿å‹¢åº«ä¸¦å„²å­˜")

    if test_query:
        retriever = vector_store.as_retriever(search_kwargs={"k": 5})
        results = retriever.invoke(test_query)
        print("ğŸ” æ¸¬è©¦æŸ¥è©¢çµæœï¼š")
        for doc in results:
            meta = doc.metadata
            print(f"- {meta['date']} | {meta['website']} | {doc.page_content}")

    return vector_store


def create_qa_function(
    model_name: str = "mistral",
    embed_model: str = "mxbai-embed-large",
    db_path: str = "./vector_db/news",
    top_k: int = 5,
    prompt_template: Optional[str] = None,
):
    """
    å»ºç«‹ä¸€å€‹ QA å‡½æ•¸ f(question: str) -> strï¼Œä½¿ç”¨æœ¬åœ°å‘é‡åº«èˆ‡ Ollama LLM å›ç­”å•é¡Œã€‚

    å›å‚³ï¼š
        qa_func: ä¸€å€‹å‡½æ•¸ï¼Œè¼¸å…¥å•é¡Œå­—ä¸²ï¼Œè¼¸å‡ºå›ç­”å­—ä¸²ã€‚
    """

    vector_store = initialize_news_vector_store(
        db_location=db_path,
        model_name=embed_model,
        test_query=None,
    )
    retriever: VectorStoreRetriever = vector_store.as_retriever(search_kwargs={"k": top_k})

    if prompt_template is None:
        prompt_template = """
ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„åŠ å¯†è²¨å¹£æ–°èåˆ†æå¸«ï¼Œæ“…é•·æ ¹æ“šæ–°èå›ç­”å•é¡Œã€‚

ä»¥ä¸‹æ˜¯ç›¸é—œæ–°èæ¨™é¡Œèˆ‡æ‘˜è¦ï¼š
{reviews}

è«‹æ ¹æ“šä»¥ä¸Šå…§å®¹å›ç­”ä»¥ä¸‹å•é¡Œï¼š
{question}
"""

    llm = OllamaLLM(model=model_name)
    prompt = ChatPromptTemplate.from_template(prompt_template)
    chain = prompt | llm

    def qa_func(question: str) -> str:
        docs = retriever.invoke(question)
        reviews = "\n".join(
            [f"- ({doc.metadata.get('date', '')}) {doc.page_content}" for doc in docs]
        )
        result = chain.invoke({"reviews": reviews, "question": question})
        
        return result.strip()

    return qa_func


# æ¸¬è©¦ç¯„ä¾‹
if __name__ == "__main__":
    qa = create_qa_function()
    while True:
        q = input("è«‹è¼¸å…¥å•é¡Œï¼ˆè¼¸å…¥ q é›¢é–‹ï¼‰ï¼š ").strip()
        if q.lower() == "q":
            break
        answer = qa(q)
        print("ğŸ§  å›ç­”ï¼š", answer)
