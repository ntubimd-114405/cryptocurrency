import os
from typing import Optional
from datetime import datetime
from news.models import Article
from langchain_ollama import OllamaEmbeddings
from langchain_chroma import Chroma
from langchain_core.documents import Document
from langchain_ollama.llms import OllamaLLM
from langchain_core.prompts import ChatPromptTemplate

def search_news(
    question: str,
    db_path: str = "./vector_db/news",
    embed_model: str = "mxbai-embed-large",
    top_k: int = 5,
    start_date: Optional[str] = None,
    end_date: Optional[str] = None,
) -> str:
    vector_store = initialize_news_vector_store(db_location=db_path, model_name=embed_model)
    # --- æ™‚é–“éæ¿¾æ¢ä»¶ ---
    where = None
    date_filters = []
    if start_date:
        start_timestamp = int(datetime.fromisoformat(start_date).timestamp())
        date_filters.append({"date": {"$gte": start_timestamp}})
    if end_date:
        end_timestamp = int(datetime.fromisoformat(end_date).timestamp())
        date_filters.append({"date": {"$lte": end_timestamp}})
    if date_filters:
        where = {"$and": date_filters} if len(date_filters) > 1 else date_filters[0]
    print(f"ğŸ” æœå°‹æ¢ä»¶ï¼š{where}")
    # --- ç›¸ä¼¼åº¦æœå°‹ ---
    docs = vector_store.similarity_search(
        query=question,
        k=top_k,
        filter=where  # ä½¿ç”¨ filter åƒæ•¸
    )
    
    results = []
    for doc in docs:
        meta = doc.metadata
        doc_id = getattr(doc, "id", None) or meta.get("id", "")
        results.append(f"(id:{doc_id}){doc.page_content}")
    
    return generate_answer("\n".join(results), question)

def generate_answer(content: str, question: str, model: str = "mistral") -> str:
    prompt = ChatPromptTemplate.from_template("""
        ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„åŠ å¯†è²¨å¹£åˆ†æé¡§å•ï¼Œè«‹å…¨ç¨‹ä½¿ç”¨ç¹é«”ä¸­æ–‡å›ç­”ã€‚

        è«‹æ ¹æ“šä»¥ä¸‹è³‡æ–™ï¼Œè©³ç´°å›ç­”é€™å€‹å•é¡Œï¼š
        {question}

        ä»¥ä¸‹æ˜¯ã€Œæ–°èæ‘˜è¦ã€çš„è³‡æ–™ï¼Œæ¯å‰‡æ–°èå‰é¢éƒ½æœ‰å°æ‡‰çš„æ–°è ID (id:xxx) å’Œæ—¥æœŸï¼Œ
        è«‹åœ¨ä½ çš„å›ç­”ä¸­æ˜ç¢ºå¼•ç”¨ç›¸é—œæ–°èçš„ IDï¼Œä¸¦æŒ‰ç…§ä»¥ä¸‹æ ¼å¼è¼¸å‡ºï¼š
        (id:xxx) (æ—¥æœŸ) - æ–°èå…§å®¹
        {content}
        """)
    llm = OllamaLLM(model=model)
    chain = prompt | llm
    result = chain.invoke({"question": question, "content": content})
    return result.strip()


def initialize_news_vector_store(
    db_location: str = "./vector_db/news",
    model_name: str = "mxbai-embed-large",
    max_docs: int = 100,
    test_query: Optional[str] = None
) -> Chroma:
    embeddings = OllamaEmbeddings(model=model_name)

    vector_store = Chroma(
        collection_name="crypto_news_articles",
        persist_directory=db_location,
        embedding_function=embeddings,
    )

    # å–å¾—å·²å­˜åœ¨çš„å‘é‡åº« ID
    existing_ids = set(vector_store.get()["ids"])

    # æ‰¾æœ€æ–°çš„æ–°è
    articles = Article.objects.filter(
        summary__isnull=False, content__isnull=False
    ).order_by("-time")[:max_docs]

    documents, ids = [], []
    for article in articles:
        if str(article.id) in existing_ids:
            continue  # é¿å…é‡è¤‡
        documents.append(Document(
            page_content=(f"{article.title}\n{article.summary}\n{article.content}")[:512],
            metadata={
                "url": article.url,
                "date": str(article.time.date()),  # ISO æ ¼å¼æ–¹ä¾¿æŸ¥è©¢
            },
            id=str(article.id)
        ))
        ids.append(str(article.id))

    if documents:
        print(f"ğŸ§  æ–°å¢ {len(documents)} ç¯‡æ–°èåˆ°å‘é‡åº«...")
        vector_store.add_documents(documents, ids=ids)
        print("âœ… å‘é‡åº«å·²æ›´æ–°")
    else:
        print("âš¡ å‘é‡åº«å·²æ˜¯æœ€æ–°ï¼Œç„¡éœ€æ›´æ–°")

    if test_query:
        retriever = vector_store.as_retriever(search_kwargs={"k": 5})
        results = retriever.invoke(test_query)
        print("ğŸ” æ¸¬è©¦æŸ¥è©¢çµæœï¼š")
        for doc in results:
            meta = doc.metadata
            print(f"- {meta.get('date')} | {meta.get('url')} | {doc.page_content}")

    return vector_store

